{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:46.401595Z",
     "start_time": "2024-08-22T08:02:46.391597Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "torch.cuda.current_device()\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# 路径\n",
    "TRAIN_PATH = './data/ccks_2017_train.csv'\n",
    "TEST_PATH = './data/ccks_2017_test.csv'\n",
    "MODEL_PATH1 = './model/'\n",
    "MODEL_PATH2 = './model/'\n",
    "MODEL_PATH_MAC = './model/'\n",
    "MODEL_LOCAL_PATH = './RoBERTa_zh_L12_PyTorch'\n",
    "\n",
    "# 超参数\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 50\n",
    "\n",
    "# 预设\n",
    "# 设备\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tag2index\n",
    "tag2index = {'O': 0,\n",
    " 'B-body': 1,\n",
    " 'I-body': 2,\n",
    " 'B-symp': 3,\n",
    " 'I-symp': 4,\n",
    " 'B-chec': 5,\n",
    " 'I-chec': 6,\n",
    " 'B-dise': 7,\n",
    " 'I-dise': 8,\n",
    " 'B-cure': 9,\n",
    " 'I-cure': 10}\n",
    "index2tag = {v: k for k, v in tag2index.items()}\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "# 预处理\n",
    "def data_preprocessing(dataset, is_train):\n",
    "    # 数据str转化为list\n",
    "    dataset['text_split'] = dataset['text'].apply(list)\n",
    "    # token\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_LOCAL_PATH)\n",
    "    texts = dataset['text_split'].array.tolist()\n",
    "    token_texts = []\n",
    "    for text in tqdm(texts):\n",
    "        tokenized = tokenizer.encode_plus(text=text,\n",
    "                                          max_length=MAX_LEN,\n",
    "                                          return_token_type_ids=True,\n",
    "                                          return_attention_mask=True,\n",
    "                                          return_tensors='pt',\n",
    "                                          padding='max_length',\n",
    "                                          truncation=True)\n",
    "        token_texts.append(tokenized)\n",
    "\n",
    "    # 训练集有tag，测试集没有tag\n",
    "    tags = None\n",
    "    if is_train:\n",
    "        dataset['tag'] = dataset['BIO_anno'].apply(lambda x: x.split(sep=' '))\n",
    "        tags = []\n",
    "        for tag in tqdm(dataset['tag'].array.tolist()):\n",
    "            index_list = [0] + [tag2index[t] for t in tag] + [0]\n",
    "            if len(index_list) < MAX_LEN:  # 填充\n",
    "                pad_length = MAX_LEN - len(index_list)\n",
    "                index_list += [tag2index['O']] * pad_length\n",
    "            if len(index_list) > MAX_LEN:  # 裁剪\n",
    "                index_list = index_list[:MAX_LEN-1] + [0]\n",
    "            tags.append(index_list)\n",
    "        tags = torch.LongTensor(tags)\n",
    "\n",
    "    return token_texts, tags\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:46.977129Z",
     "start_time": "2024-08-22T08:02:46.967156Z"
    }
   },
   "id": "c4522a7e089abd42",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Bert_BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, tag2index):\n",
    "        super(Bert_BiLSTM_CRF, self).__init__()\n",
    "        self.tagset_size = len(tag2index)\n",
    "\n",
    "        # bert层\n",
    "        self.bert = BertModel.from_pretrained(MODEL_LOCAL_PATH)\n",
    "        # config = self.bert.config\n",
    "        # lstm层\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        # Dense层\n",
    "        self.dense = nn.Linear(in_features=256, out_features=self.tagset_size)\n",
    "        # CRF层\n",
    "        self.crf = CRF(num_tags=self.tagset_size)\n",
    "\n",
    "        # 隐藏层\n",
    "        self.hidden = None\n",
    "\n",
    "    # 负对数似然损失函数\n",
    "    def neg_log_likelihood(self, emissions, tags=None, mask=None, reduction=None):\n",
    "        return -1 * self.crf(emissions=emissions, tags=tags, mask=mask, reduction=reduction)\n",
    "\n",
    "    def forward(self, token_texts, tags):\n",
    "        \"\"\"\n",
    "        token_texts:{\"input_size\": tensor,  [batch, 1, seq_len]->[batch, seq_len]\n",
    "                    \"token_type_ids\": tensor,  [batch, 1, seq_len]->[batch, seq_len]\n",
    "                     \"attention_mask\": tensor  [batch, 1, seq_len]->[batch, seq_len]->[seq_len, batch]\n",
    "                     }\n",
    "        tags:  [batch, seq_len]->[seq_len, batch]\n",
    "        bert_out:  [batch, seq_len, hidden_size(768)]->[seq_len, batch, hidden_size]\n",
    "        self.hidden:  [num_layers * num_directions, hidden_size(128)]\n",
    "        out:  [seq_len, batch, hidden_size * 2(256)]\n",
    "        lstm_feats:  [seq_len, batch, tagset_size]\n",
    "        loss:  tensor\n",
    "        predictions:  [batch, num]\n",
    "        \"\"\"\n",
    "        texts, token_type_ids, masks = token_texts['input_ids'], token_texts['token_type_ids'], token_texts['attention_mask']\n",
    "        texts = texts.squeeze(1)\n",
    "        token_type_ids = token_type_ids.squeeze(1)\n",
    "        masks = masks.squeeze(1)\n",
    "        bert_out = self.bert(input_ids=texts, attention_mask=masks, token_type_ids=token_type_ids)[0]\n",
    "        bert_out = bert_out.permute(1, 0, 2)\n",
    "        # 检测设备\n",
    "        device = bert_out.device\n",
    "        # 初始化隐藏层参数\n",
    "        self.hidden = (torch.randn(2, bert_out.size(0), 128).to(device),\n",
    "                       torch.randn(2, bert_out.size(0), 128).to(device))\n",
    "        out, self.hidden = self.lstm(bert_out, self.hidden)\n",
    "        lstm_feats = self.dense(out)\n",
    "\n",
    "        # 格式转换\n",
    "        masks = masks.permute(1, 0)\n",
    "        masks = masks.clone().detach().bool()\n",
    "        # masks = torch.tensor(masks, dtype=torch.uint8)\n",
    "        # 计算损失值和预测值\n",
    "        if tags is not None:\n",
    "            tags = tags.permute(1, 0)\n",
    "            loss = self.neg_log_likelihood(lstm_feats, tags, masks, 'mean')\n",
    "            predictions = self.crf.decode(emissions=lstm_feats, mask=masks)  # [batch, 任意数]\n",
    "            return loss, predictions\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions=lstm_feats, mask=masks)\n",
    "            return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:47.585244Z",
     "start_time": "2024-08-22T08:02:47.560312Z"
    }
   },
   "id": "14c2742ce1626da1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, token_texts, tags):\n",
    "        super(NerDataset, self).__init__()\n",
    "        self.token_texts = token_texts\n",
    "        self.tags = tags\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"token_texts\": self.token_texts[index],\n",
    "            \"tags\": self.tags[index] if self.tags is not None else None,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_texts)\n",
    "\n",
    "\n",
    "class NerDatasetTest(Dataset):\n",
    "    def __init__(self, token_texts):\n",
    "        super(NerDatasetTest, self).__init__()\n",
    "        self.token_texts = token_texts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"token_texts\": self.token_texts[index],\n",
    "            \"tags\": 0\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:48.211885Z",
     "start_time": "2024-08-22T08:02:48.196925Z"
    }
   },
   "id": "7d8cd595fd5565ea",
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class Bert_CRF(nn.Module):\n",
    "    def __init__(self, tag2index):\n",
    "        super(Bert_CRF, self).__init__()\n",
    "        self.tagset_size = len(tag2index)\n",
    "\n",
    "        # bert层\n",
    "        self.bert = BertModel.from_pretrained(MODEL_LOCAL_PATH)\n",
    "        # dense层\n",
    "        self.dense = nn.Linear(in_features=768, out_features=self.tagset_size)\n",
    "        # CRF层\n",
    "        self.crf = CRF(num_tags=self.tagset_size)\n",
    "\n",
    "        # 隐藏层\n",
    "        self.hidden = None\n",
    "\n",
    "    def neg_log_likelihood(self, emissions, tags=None, mask=None, reduction=None):\n",
    "        return -1 * self.crf(emissions=emissions, tags=tags, mask=mask, reduction=reduction)\n",
    "\n",
    "    def forward(self, token_texts, tags):\n",
    "        \"\"\"\n",
    "        token_texts:{\"input_size\": tensor,  [batch, 1, seq_len]->[batch, seq_len]\n",
    "                    \"token_type_ids\": tensor,  [batch, 1, seq_len]->[batch, seq_len]\n",
    "                     \"attention_mask\": tensor  [batch, 1, seq_len]->[batch, seq_len]->[seq_len, batch]\n",
    "                     }\n",
    "        tags:  [batch, seq_len]->[seq_len, batch]\n",
    "        bert_out:  [batch, seq_len, hidden_size(768)]->[seq_len, batch, hidden_size]\n",
    "        feats:  [seq_len, batch, tagset_size]\n",
    "        loss:  tensor\n",
    "        predictions:  [batch, num]\n",
    "        \"\"\"\n",
    "        texts, token_type_ids, masks = token_texts.values()\n",
    "        texts = texts.squeeze(1)\n",
    "        token_type_ids = token_type_ids.squeeze(1)\n",
    "        masks = masks.squeeze(1)\n",
    "        bert_out = self.bert(input_ids=texts, attention_mask=masks, token_type_ids=token_type_ids)[0]\n",
    "        bert_out = bert_out.permute(1, 0, 2)\n",
    "        feats = self.dense(bert_out)\n",
    "\n",
    "        # 格式转换\n",
    "        masks = masks.permute(1, 0)\n",
    "        masks = masks.clone().detach().bool()\n",
    "        # 计算损失之和预测值\n",
    "        if tags is not None:\n",
    "            tags = tags.permute(1, 0)\n",
    "            loss = self.neg_log_likelihood(feats, tags, masks, 'mean')\n",
    "            predictions = self.crf.decode(emissions=feats, mask=masks)\n",
    "            return loss, predictions\n",
    "        else:\n",
    "            predictions = self.crf.decode(emissions=feats, mask=masks)\n",
    "            return predictions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:48.743958Z",
     "start_time": "2024-08-22T08:02:48.722459Z"
    }
   },
   "id": "7ae5a50551b06903",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import precision_score, recall_score\n",
    "\n",
    "# 计算f1值\n",
    "def get_f1_score(tags, predictions):\n",
    "    tags = tags.to('cpu').data.numpy().tolist()\n",
    "    temp_tags = []\n",
    "    final_tags = []\n",
    "\n",
    "    int_tags = []\n",
    "    f_int_tags = []\n",
    "    predictions_int = []\n",
    "    for index in range(len(predictions)):\n",
    "        # predictions先去掉头，再去掉尾\n",
    "        predictions[index].pop()\n",
    "        length = len(predictions[index])\n",
    "        temp_tags.append(tags[index][1:length])\n",
    "        int_tags.append(tags[index][1:length])\n",
    "\n",
    "        predictions[index].pop(0)\n",
    "        predictions_int = predictions.copy()\n",
    "        # 格式转化，转化为List(str)\n",
    "        temp_tags[index] = [index2tag[x] for x in temp_tags[index]]\n",
    "        predictions[index] = [index2tag[x] for x in predictions[index]]\n",
    "\n",
    "        final_tags.append(temp_tags[index])\n",
    "        f_int_tags.append(int_tags[index])\n",
    "\n",
    "\n",
    "    f1 = f1_score(final_tags, predictions, average='micro')\n",
    "    precision = 0\n",
    "    predcision = precision_score(final_tags, predictions)\n",
    "    recall =  recall_score(final_tags, predictions)\n",
    "\n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'predcision': predcision,\n",
    "        'micro_f1': f1,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:49.695863Z",
     "start_time": "2024-08-22T08:02:49.678959Z"
    }
   },
   "id": "fe6206cea9bdbc83",
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "# 训练\n",
    "def train(train_dataloader, model, optimizer, epoch):\n",
    "    for i, batch_data in enumerate(train_dataloader):\n",
    "        token_texts = batch_data['token_texts'].to(DEVICE)\n",
    "        tags = batch_data['tags'].to(DEVICE)\n",
    "        loss, predictions = model(token_texts, tags)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            f1_obj = get_f1_score(tags, predictions)\n",
    "            micro_f1 = f1_obj['micro_f1']\n",
    "            predcision = f1_obj['predcision']\n",
    "            recall = f1_obj['recall']\n",
    "            print(f'Epoch:{epoch} | i:{i} | loss:{loss.item()} | Micro_F1:{micro_f1} | predcision: {predcision} | recall: {recall}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:02:52.706614Z",
     "start_time": "2024-08-22T08:02:52.700629Z"
    }
   },
   "id": "6e495d1a7ab4c19a",
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = pd.read_csv(TRAIN_PATH, encoding='utf8')\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T06:26:06.603083Z",
     "start_time": "2024-08-22T06:26:06.558076Z"
    }
   },
   "id": "f4626d8c7c2ebf45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  明，女性，26岁，河北省承德市滦平县人，现住河北省承德市双滦区御水花园，主因头部、腹部双下肢...   \n",
       "1              1  ，中年男性，32岁，生于河北省承德市，现住河北省承德市，主因左肘关节屈曲畸形14个月于201...   \n",
       "2              2  ，青年男性，25岁，河北省承德市双滦区人，现住河北省承德市双滦区三岔口御景家园，主因外伤后头...   \n",
       "3              3  神清语利，自动体位，查体合作。左、右上中切牙缺失，右上侧切牙松动、触痛，下唇右侧内黏膜挫伤，...   \n",
       "4              4            p；nbsp；右腰部疼痛伴发热12小时于2016--10--213：13收入院   \n",
       "...          ...                                                ...   \n",
       "1555        1555  患者入院后完善相关辅助检查，根据患者出现周身无力，纳差等恶病质表现，查体肝位于右侧季肋下2c...   \n",
       "1556        1556  患者入院后完善相关辅助检查，依据患者依据活动后胸闷、气短12年，间断喘憋2年，加重2天入院。...   \n",
       "1557        1557  入院后依据病史、症状、体征及辅助检查诊断为：1.颈椎病。2.高血压病。3.缺血性心肌病。入院...   \n",
       "1558        1558  依据患者病情、症状、体征及辅助检查诊断为：阵发性室上性心动过速。治疗上给予：1.予以内科II...   \n",
       "1559        1559  入院后检查各项辅助检查，据患者发作性胸痛，每次发作持续3-5分钟，休息或口服硝酸甘油可缓解，...   \n",
       "\n",
       "                                               BIO_anno  \n",
       "0     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "1     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "2     O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "3     O O O O O O O O O O O O O O O B-body I-body I-...  \n",
       "4     O O O O O O O B-body I-body I-body B-symp I-sy...  \n",
       "...                                                 ...  \n",
       "1555  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "1556  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "1557  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "1558  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "1559  O O O O O O O O O O O O O O O O O O B-symp I-s...  \n",
       "\n",
       "[1560 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>明，女性，26岁，河北省承德市滦平县人，现住河北省承德市双滦区御水花园，主因头部、腹部双下肢...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>，中年男性，32岁，生于河北省承德市，现住河北省承德市，主因左肘关节屈曲畸形14个月于201...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>，青年男性，25岁，河北省承德市双滦区人，现住河北省承德市双滦区三岔口御景家园，主因外伤后头...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>神清语利，自动体位，查体合作。左、右上中切牙缺失，右上侧切牙松动、触痛，下唇右侧内黏膜挫伤，...</td>\n",
       "      <td>O O O O O O O O O O O O O O O B-body I-body I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>p；nbsp；右腰部疼痛伴发热12小时于2016--10--213：13收入院</td>\n",
       "      <td>O O O O O O O B-body I-body I-body B-symp I-sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>1555</td>\n",
       "      <td>患者入院后完善相关辅助检查，根据患者出现周身无力，纳差等恶病质表现，查体肝位于右侧季肋下2c...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>1556</td>\n",
       "      <td>患者入院后完善相关辅助检查，依据患者依据活动后胸闷、气短12年，间断喘憋2年，加重2天入院。...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>1557</td>\n",
       "      <td>入院后依据病史、症状、体征及辅助检查诊断为：1.颈椎病。2.高血压病。3.缺血性心肌病。入院...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>1558</td>\n",
       "      <td>依据患者病情、症状、体征及辅助检查诊断为：阵发性室上性心动过速。治疗上给予：1.予以内科II...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>1559</td>\n",
       "      <td>入院后检查各项辅助检查，据患者发作性胸痛，每次发作持续3-5分钟，休息或口服硝酸甘油可缓解，...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O B-symp I-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1560 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = pd.read_csv(TRAIN_PATH, encoding='utf8')\n",
    "# 数据预处理\n",
    "token_texts, tags = data_preprocessing(train_dataset, is_train=True)\n",
    "# 数据集装载\n",
    "\n",
    "tags[20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T06:26:10.815933Z",
     "start_time": "2024-08-22T06:26:09.859330Z"
    }
   },
   "id": "dfea626f12a1c37f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:00<00:00, 2421.15it/s]\n",
      "100%|██████████| 1560/1560 [00:00<00:00, 50413.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 3, 4, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "token_texts[20]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T06:26:12.150139Z",
     "start_time": "2024-08-22T06:26:12.136828Z"
    }
   },
   "id": "ce10f52a714c7912",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1381, 2797, 1912,  839, 1400, 4563, 4578,  845, 1928, 3238,  122,\n",
       "         1921,  754,  123,  121,  122,  127,  118,  118,  121,  130,  118,  118,\n",
       "          121,  125,  122,  121, 8038,  126,  123, 3119, 1057, 7368,  511,  102,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "token_texts[20]['attention_mask'].shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T06:26:12.696977Z",
     "start_time": "2024-08-22T06:26:12.686007Z"
    }
   },
   "id": "5442773a5a09e3aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "def execute():\n",
    "    # 加载训练集\n",
    "    train_dataset = pd.read_csv(TRAIN_PATH, encoding='utf8')\n",
    "    # 数据预处理\n",
    "    token_texts, tags = data_preprocessing(train_dataset, is_train=True)\n",
    "    # 数据集装载\n",
    "    train_dataset = NerDataset(token_texts, tags)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    # 构建模型\n",
    "    #model = Bert_BiLSTM_CRF(tag2index=tag2index).to(DEVICE)\n",
    "    model = Bert_CRF(tag2index=tag2index).to(DEVICE)\n",
    "    # 初始化模型参数优化器\n",
    "\n",
    "    crf_params = list(map(id, model.crf.parameters()))\n",
    "\n",
    "    base_params = filter(lambda p: id(p) not in crf_params,\n",
    "                     model.parameters())\n",
    "\n",
    "    optimizer_params = [\n",
    "          {'params': base_params},\n",
    "          {'params': model.crf.parameters(), 'lr': 2e-5 * 100},\n",
    "    ]\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_params, lr=2e-5)\n",
    "    print(f\"GPU_NAME:{torch.cuda.get_device_name()} | Memory_Allocated:{torch.cuda.memory_allocated()}\")\n",
    "    # 模型训练\n",
    "    for i in range(EPOCH):\n",
    "        print(f\"{i} Epoch\")\n",
    "        train(train_dataloader, model, optimizer, i)\n",
    "        # 保存模型\n",
    "        torch.save(model.state_dict(), MODEL_PATH2 + str(i) + '.pkl')\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), MODEL_PATH_MAC + 'final.pkl')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T08:03:13.362716Z",
     "start_time": "2024-08-22T08:03:13.345765Z"
    }
   },
   "id": "c4249518bfbb53af",
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "#测试集预测实体标签\n",
    "def test(model_path, model_id):\n",
    "    # 加载数据集\n",
    "    test_dataset = pd.read_csv(TEST_PATH, encoding='utf8')\n",
    "    #测试集的tags_list\n",
    "    test_tags_true = test_dataset['BIO_anno'].apply(lambda x: x.split(sep=' '))\n",
    "    test_tags_true = test_tags_true.array.tolist()\n",
    "    # 数据预处理\n",
    "    token_texts, _ = data_preprocessing(test_dataset, is_train=False)\n",
    "    # 装载测试集\n",
    "    dataset_test = NerDatasetTest(token_texts)\n",
    "    test_dataloader = DataLoader(dataset=dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    # 构建模型\n",
    "    #model = Bert_BiLSTM_CRF(tag2index).to(DEVICE)\n",
    "    model = Bert_CRF(tag2index).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # 模型预测\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(test_dataloader):\n",
    "            token_texts = batch_data['token_texts'].to(DEVICE)\n",
    "            predictions = model(token_texts, None)\n",
    "            predictions_list.extend(predictions)\n",
    "\n",
    "    # 将预测结果转换为文本格式\n",
    "    entity_tag_list = []\n",
    "    result = []\n",
    "    index2tag = {v: k for k, v in tag2index.items()}  # 反转字典\n",
    "    for i, (text, predictions) in enumerate(zip(test_dataset['text'], predictions_list)):\n",
    "        # 删除首位和最后一位\n",
    "        predictions.pop()\n",
    "        predictions.pop(0)\n",
    "        text_entity_tag = []\n",
    "        result_tag = []\n",
    "        for c, t in zip(text, predictions):\n",
    "            if t != 0:\n",
    "                text_entity_tag.append(c + index2tag[t])\n",
    "                result_tag.append(index2tag[t])\n",
    "            else:\n",
    "                result_tag.append('O')\n",
    "        result.append(result_tag)\n",
    "        entity_tag_list.append(\" \".join(text_entity_tag))  # 合并为str并加入列表中\n",
    "\n",
    "    result_df = pd.DataFrame(data=entity_tag_list, columns=['result'])\n",
    "    result_df.to_csv('./result_20240822/result.csv')\n",
    "\n",
    "    for t in range(len(test_tags_true)):\n",
    "      if len(test_tags_true[t]) != len(result[t]):\n",
    "        test_tags_true[t] = test_tags_true[t][0: len(result[t])]\n",
    "    \n",
    "    predcision = precision_score(test_tags_true, result)\n",
    "    recall =  recall_score(test_tags_true, result)\n",
    "    class_report = classification_report(test_tags_true, result, digits=6)\n",
    "\n",
    "    print(f\"{model_id} test f1_score = \" + str(f1_score(test_tags_true, result)))\n",
    "    print(f\"{model_id} test predcision_score = \" + str(predcision))\n",
    "    print(f\"{model_id} test recall_score = \" + str(recall))\n",
    "    print(f\"{model_id} test report = \" + class_report)\n",
    "    \n",
    "    return {\n",
    "        \"f1\": f1_score(test_tags_true, result),\n",
    "        \"pred\": predcision,\n",
    "        'recall': recall,\n",
    "        'report': class_report,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T14:44:21.221900Z",
     "start_time": "2024-08-22T14:44:21.209932Z"
    }
   },
   "id": "4696eade610d72c7",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
